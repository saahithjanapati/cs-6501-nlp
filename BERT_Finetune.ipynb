{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d0b1268-91e0-4360-b622-6e0e5fdaf54a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c30b43791a46bdacdcb1999cb85ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2833d7050d24f6b866447f24178b580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dacd02d2da845359bfd5962a539fd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jax4zk/.local/lib/python3.11/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='154' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [154/219 00:09 < 00:03, 16.31 it/s, Epoch 0.70/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 68\u001b[0m\n\u001b[1;32m     58\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     59\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     60\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     65\u001b[0m )\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Evaluate on the test set\u001b[39;00m\n\u001b[1;32m     71\u001b[0m test_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(test_dataset)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:2273\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2274\u001b[0m ):\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(\"trim.csv\")  # Replace with your file name\n",
    "assert \"title\" in data.columns and \"label\" in data.columns\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.seed(42)\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Manually split the dataset\n",
    "train_data = data.iloc[:3500]\n",
    "val_data = data.iloc[3500:4000]\n",
    "test_data = data.iloc[4000:]\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    # Ensure the \"title\" field is a string\n",
    "    examples[\"title\"] = [str(title) for title in examples[\"title\"]]\n",
    "    return tokenizer(examples[\"title\"], truncation=True, padding=True)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data).map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "val_dataset = Dataset.from_pandas(val_data).map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "test_dataset = Dataset.from_pandas(test_data).map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "\n",
    "# Define the model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=data['label'].nunique())\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = (labels == preds).mean()\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(\"Test results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c6d46-6c9f-47ae-aec4-6be4f9dff871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count the distribution of labels in train_data\n",
    "train_label_distribution = train_data['label'].value_counts()\n",
    "\n",
    "# Count the distribution of labels in val_data\n",
    "val_label_distribution = val_data['label'].value_counts()\n",
    "\n",
    "# Count the distribution of labels in val_data\n",
    "test_label_distribution = test_data['label'].value_counts()\n",
    "\n",
    "\n",
    "# Display the distributions\n",
    "print(\"Train Data Label Distribution:\")\n",
    "print(train_label_distribution)\n",
    "\n",
    "print(\"\\nValidation Data Label Distribution:\")\n",
    "print(val_label_distribution)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\Test Data Label Distribution:\")\n",
    "print(test_label_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f6de3-3738-4774-af92-44f11070fe8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for data in test_data['title']:\n",
    "    print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1dfdba-9ed8-4eba-b784-4310019f81c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df008a7-04b2-4648-87e9-f043c9a2e46f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to extract embeddings for the test dataset\n",
    "def extract_embeddings():\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    # BERT requires inputs in specific formats\n",
    "    for sample in test_dataset:\n",
    "        \n",
    "        \n",
    "        inputs = tokenizer(sample['text'], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "        # Extract the embedding of the classification token ([CLS])\n",
    "        # print(len(outputs['hidden_states']))\n",
    "        cls_embedding = outputs['hidden_states'][-1][:, 0, :].squeeze().to('cpu').numpy()\n",
    "        # cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "        embeddings.append(cls_embedding)\n",
    "        labels.append(int(sample[\"label\"]))  # Store labels for visualization\n",
    "    \n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fe30fb-b9f9-44c9-8984-22cfb13d1ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract embeddings and labels from the test dataset\n",
    "test_embeddings, test_labels = extract_embeddings()\n",
    "\n",
    "# Reduce dimensionality using PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(test_embeddings)\n",
    "\n",
    "# Ensure test_labels has two distinct classes\n",
    "assert len(set(test_labels)) == 2, \"Test labels must have exactly two classes for this visualization.\"\n",
    "\n",
    "# Assign colors based on the two classes\n",
    "class_colors = {0: 'red', 1: 'blue'}  # Adjust these colors as needed\n",
    "point_colors = [class_colors[label] for label in test_labels]\n",
    "\n",
    "# Plot PCA visualization\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=point_colors, alpha=0.7)\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', label=f'Class {cls} {\" - Fake\" if cls == 1 else \"- Real\"}', markersize=10, markerfacecolor=col)\n",
    "           for cls, col in class_colors.items()]\n",
    "plt.legend(handles=handles, title=\"Classes\")\n",
    "plt.title(\"PCA Visualization of [CLS] Token Embeddings\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.savefig(\"PCA_Bert_Viz_Classes.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e9e11-fa5c-4c82-a5ae-13e9574b0229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ensure test_labels has two distinct classes\n",
    "assert len(set(test_labels)) == 2, \"Test labels must have exactly two classes for this visualization.\"\n",
    "\n",
    "# Reduce dimensionality using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "tsne_result = tsne.fit_transform(np.array(test_embeddings))\n",
    "\n",
    "# Assign colors based on the two classes\n",
    "class_colors = {0: 'red', 1: 'blue'}  # Adjust these colors as needed\n",
    "point_colors = [class_colors[label] for label in test_labels]\n",
    "\n",
    "# Plot t-SNE visualization\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=point_colors, alpha=0.7)\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', label=f'Class {cls} {\" - Fake\" if cls == 1 else \"- Real\"}', markersize=10, markerfacecolor=col)\n",
    "           for cls, col in class_colors.items()]\n",
    "plt.legend(handles=handles, title=\"Classes\")\n",
    "plt.title(\"t-SNE Visualization of [CLS] Token Embeddings\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.savefig(\"t-SNE_BERT_Viz_Classes.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5eb89-4ec5-4dcb-8f64-19851020facd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
